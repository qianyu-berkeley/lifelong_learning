{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**odds, odds ratio, and probability**\n",
    "\n",
    "$$odds(p) = (\\frac{p}{1-p})$$\n",
    "$$odds\\ ratio = \\frac{\\frac{X_A}{X_B}}{\\frac{Y_A}{Y_B}}$$\n",
    "$$relative\\_risk = \\frac{\\frac{X_A}{X_A+Y_A}}{\\frac{X_B}{X_B+Y_B}}$$\n",
    "where X is treated, Y is control, A is impacted, B is not impacted\n",
    "$$ probability = \\frac{odds}{1+odds} = \\frac{4}{1+4} = 0.8$$\n",
    "\n",
    "**Distribution of logistic regression predictor and outcome variables**\n",
    "\n",
    "$$Z = logit(P) = log(odds) = log(\\frac{P}{1-P}) = \\theta^Tx = \\theta_0 + \\theta_1$$\n",
    "$$e^Z = \\frac{P}{1-P}$$\n",
    "$$P = \\frac{e^Z}{1+e^Z} = \\frac{1}{1+e^{-Z}}$$\n",
    "\n",
    "**Sigmoid function (logistic function for binary classification and a neuron activation function)**\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-\\theta x}}$$\n",
    "\n",
    "**Derivative of sigmoid funtion (we can expand this to softmax)**\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x)=\\frac{e^{-x}}{(1+e^{-x})^2}$$\n",
    "or \n",
    "$$\\sigma'(x) = \\sigma(x)(1-\\sigma(x)) $$ \n",
    "\n",
    "**Logistic Regression Definition (put the above concept together)**\n",
    "\n",
    "* Hypothesis function $h_{\\theta}(x)$\n",
    "  Logit: $Z = \\theta^Tx$\n",
    "  $$h_{\\theta}(x) = \\frac{1}{1+e^Z} = \\frac{1}{1+e^{-\\theta^T x}}$$\n",
    "\n",
    "* Decision Boundry:\n",
    "  $$h_{\\theta}(x) \\geq 0.5  \\to y = 1$$\n",
    "  $$h_{\\theta}(x) < 0.5  \\to y = 0$$\n",
    "  or\n",
    "  $$\\theta^T \\geq 0 \\to y = 1$$\n",
    "  $$\\theta^T < 0 \\to y = 0$$\n",
    "\n",
    "* Cost Function (Measure the goodness of our hypothesis with respect to all data samples)\n",
    "  $$J(\\theta) = \\frac{1}{m} \\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^(i))$$\n",
    "  $$J(\\theta) = \\frac{1}{m} \\sum^m_{i=1}(-y^ilog(h_\\theta(x^i)) - (1-y^i)log(1-h_\\theta(x^i)) )$$\n",
    "  $$J(\\theta) = -\\frac{1}{m} \\sum^m_{i=1}(y^ilog(h_\\theta(x^i)) + (1-y^i)log(1-h_\\theta(x^i)) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def logit(P):\n",
    "    return log(P/(1-P))\n",
    "\n",
    "def sigmoid(p, x):\n",
    "    Z = -1*(p.T@x)\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def d_sigmoid(p, x):\n",
    "    return sigmoid(p, x)*(1 - sigmoid(p, x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Programming and Bayesian DL\n",
    "\n",
    "* Bayesian Statistics\n",
    "    * Beta Distribution\n",
    "    * Binomial likelihood\n",
    "* Probability Theory\n",
    "* Probabilistic library (PyMc3, Stan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "  * PMI (Pointwise Mutual Inforamtion): how much knowing one outcome tells you about another\n",
    "    * $$\\text{PMI}(x, y) = \\log_2\\frac{p(x, y)}{p(x)\\ p(y)}$$\n",
    "    * if x, y are indepdent, PMI = 0 as P(x, y) = 0\n",
    "\n",
    "  * Entropy (Shannon entropy) is how 'uncertain' the outcome of some experiment is. \n",
    "    * The more uncertain the more spread out the disbribution, the higher the entropy\n",
    "    * $$\\text{Entropy}(X) = H(X) = -\\Sigma_x\\ p(x) \\log_2 p(x)$$\n",
    "    * to find expected value $E[log_2{p(x)}]$ for the probability distribution\n",
    "    * Example: BinaryEntropy (coin flip)\n",
    "      * BinaryEntripy(p = 0) = 0.0 always get tail, no uncertainty\n",
    "      * BinaryEntropy(p = 0.5) = 1.0 max uncertainty (note that entropy value can be infinitely large)\n",
    "    * entropy is the average number of bits per message going across the wire given that you optimally designed your encoding ($-log_2P(x)$) in general case\n",
    "  * Cross Entropy\n",
    "    * The expected value for the number of bits you'd put on the wire in the case where you send messages with probability $P(X)$ but designed an optimal code with $Q(X)$\n",
    "    * $H(X) = CrossEntropy(P, Q) = -\\sum_x P(X) log_2 Q(x)$\n",
    "    * crossEntropy is 0 if P match Q (prediction match the class)\n",
    "  * KL Divergence\n",
    "    * the size of the *penalty* for using the wrong distribution to optimize our code).  That difference is known as the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), or KL divergence for short.\n",
    "    * It is a measure of how different two probability distributions are.  The more $Q$ differs from $P$, the worse the penalty would be, and thus the higher the KL divergence.\n",
    "    * $ D_{KL}(P\\ ||\\ Q) = CE(P, Q) - H(P)$\n",
    "    * $D_{KL}(P\\ ||\\ Q) \\ne D_{KL}(Q\\ ||\\ P)$ not symmetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "2 import numpy\n",
    "3 print (math.log(1.0/0.98)) # Natural log (ln)\n",
    "4 print (numpy.log(1.0/0.02)) # Natural log (ln)\n",
    "5\n",
    "6 print (math.log10(1.0/0.98)) # Common log (base 10)\n",
    "7 print (numpy.log10(1.0/0.02)) # Common log (base 10)\n",
    "8\n",
    "9 print (math.log2(1.0/0.98)) # Binary log (base 2)\n",
    "10 print (numpy.log2(1.0/0.02)) # Binary log ("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
